{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7847b3-9834-4193-92a3-9144ffcfc1ba",
   "metadata": {},
   "source": [
    "We are goingto perform a model compression technique known as knowledge distillation in which a student model \"learns\" from a more complex model known as the teacher. In particular you will:\n",
    "\n",
    "Define a Distiller class with the custom logic for the distillation process.\n",
    "Train the teacher model which is a CNN that implements regularization via dropout.\n",
    "Train a student model (a smaller version of the teacher without regularization) by using knowledge distillation.\n",
    "Train another student model from scratch without distillation called student_scratch.\n",
    "Compare the three students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3463a0-bc98-4aa0-af56-eac4a54782fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For setting random seeds\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(42)\n",
    "\n",
    "# Libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# More random seed setup\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455b685-bb86-41b7-bfc6-aab9eda8c35c",
   "metadata": {},
   "source": [
    "Use cats vs Dogs datasets as it has a lot of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f06441-dc7f-4fc8-b0d7-6101565c5eba",
   "metadata": {},
   "source": [
    "Begin by downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2006c5a2-9bd8-4a16-9575-d4468b6320a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Lenovo\\tensorflow_datasets\\cats_vs_dogs\\4.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83540490023f47558e0b0454f6c9bf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc1c5dfa80d4a18b7cdf97d47b8a4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:1738 images were corrupted and were skipped\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Lenovo\\tensorflow_datasets\\cats_vs_dogs\\4.0.0.incompleteHBRPOI\\cats_vs_dogs-train.tfrecord*â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cats_vs_dogs downloaded and prepared to C:\\Users\\Lenovo\\tensorflow_datasets\\cats_vs_dogs\\4.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "There are 23262 images for 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define train/test splits\n",
    "splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n",
    "\n",
    "# Download the dataset\n",
    "(train_examples, validation_examples, test_examples), info = tfds.load('cats_vs_dogs', with_info=True, as_supervised=True, split=splits)\n",
    "\n",
    "# Print useful information\n",
    "num_examples = info.splits['train'].num_examples\n",
    "num_classes = info.features['label'].num_classes\n",
    "\n",
    "print(f\"There are {num_examples} images for {num_classes} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60374114-c020-4931-bc43-7fc9684400c9",
   "metadata": {},
   "source": [
    "Preprocess the data for training by normalizing pixel values, reshaping them and creating batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bd1d9d-d4a1-447f-a00a-c67ae2d8fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global variables\n",
    "pixels = 224\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Apply resizing and pixel normalization\n",
    "def format_image(image, label):\n",
    "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
    "    return  image, label\n",
    "\n",
    "# Create batches of data\n",
    "train_batches = train_examples.shuffle(num_examples // 64).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
    "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
    "test_batches = test_examples.map(format_image).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2170741a-1351-4bee-b46d-2e6c4692fdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fe890-fe06-448a-8b59-cac2e20ac87b",
   "metadata": {},
   "source": [
    "Code the custom Distiller model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37688a-ea13-49e8-88d0-97b9ee59a8c0",
   "metadata": {},
   "source": [
    "In order to implement the distillation process, we will create a custom Keras model with name as Distiller and we will overwrite some of the methods such as\n",
    "1. compile: This model needs some extra parameters to be compiled such as the teacher and student losses, the alpha and the temperature.\n",
    "2. train_step: Controls how the model is trained. This will be where the actual knowledge distillation logic will be found. This method is what is called when you do model.fit.\n",
    "3. test_step: Controls the evaluation of the model. This method is what is called when you do model.evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc23c2f-9174-48a5-97e5-950ee10f4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "\n",
    "  # Needs both the student and teacher models to create an instance of this class\n",
    "  def __init__(self, student, teacher):\n",
    "      super(Distiller, self).__init__()\n",
    "      self.teacher = teacher\n",
    "      self.student = student\n",
    "\n",
    "\n",
    "  # Will be used when calling model.compile()\n",
    "  def compile(self, optimizer, metrics, student_loss_fn,\n",
    "              distillation_loss_fn, alpha, temperature):\n",
    "\n",
    "      # Compile using the optimizer and metrics\n",
    "      super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "      \n",
    "      # Add the other params to the instance\n",
    "      self.student_loss_fn = student_loss_fn\n",
    "      self.distillation_loss_fn = distillation_loss_fn\n",
    "      self.alpha = alpha\n",
    "      self.temperature = temperature\n",
    "\n",
    "\n",
    "  # Will be used when calling model.fit()\n",
    "  def train_step(self, data):\n",
    "      # Data is expected to be a tuple of (features, labels)\n",
    "      x, y = data\n",
    "\n",
    "      # Vanilla forward pass of the teacher\n",
    "      # Note that the teacher is NOT trained\n",
    "      teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "      # Use GradientTape to save gradients\n",
    "      with tf.GradientTape() as tape:\n",
    "          # Vanilla forward pass of the student\n",
    "          student_predictions = self.student(x, training=True)\n",
    "\n",
    "          # Compute vanilla student loss\n",
    "          student_loss = self.student_loss_fn(y, student_predictions)\n",
    "          \n",
    "          # Compute distillation loss\n",
    "          # Should be KL divergence between logits softened by a temperature factor\n",
    "          distillation_loss = self.distillation_loss_fn(\n",
    "              tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "              tf.nn.softmax(student_predictions / self.temperature, axis=1))\n",
    "\n",
    "          # Compute loss by weighting the two previous losses using the alpha param\n",
    "          loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "      # Use tape to calculate gradients for student\n",
    "      trainable_vars = self.student.trainable_variables\n",
    "      gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "      # Update student weights \n",
    "      # Note that this done ONLY for the student\n",
    "      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "      # Update the metrics\n",
    "      self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "      # Return a performance dictionary\n",
    "      # You will see this being outputted during training\n",
    "      results = {m.name: m.result() for m in self.metrics}\n",
    "      results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss})\n",
    "      return results\n",
    "\n",
    "\n",
    "  # Will be used when calling model.evaluate()\n",
    "  def test_step(self, data):\n",
    "      # Data is expected to be a tuple of (features, labels)\n",
    "      x, y = data\n",
    "\n",
    "      # Use student to make predictions\n",
    "      # Notice that the training param is set to False\n",
    "      y_prediction = self.student(x, training=False)\n",
    "\n",
    "      # Calculate student's vanilla loss\n",
    "      student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "      # Update the metrics\n",
    "      self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "      # Return a performance dictionary\n",
    "      # You will see this being outputted during inference\n",
    "      results = {m.name: m.result() for m in self.metrics}\n",
    "      results.update({\"student_loss\": student_loss})\n",
    "      return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bb858f-f3dc-4dec-b9ef-14947a7a965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher model\n",
    "def create_big_model():\n",
    "  tf.random.set_seed(42)\n",
    "  model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(2)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "# Student model\n",
    "def create_small_model():\n",
    "  tf.random.set_seed(42)\n",
    "  model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(2)\n",
    "  ])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f08bf-7b26-4470-be19-8f2f7bb4795b",
   "metadata": {},
   "source": [
    "There are two important things to notice:\n",
    "\n",
    "The last layer does not have an softmax activation because the raw logits are needed for the knowledge distillation.\n",
    "Regularization via dropout layers will be applied to the teacher but NOT to the student. This is because the student should be able to learn this regularization through the distillation process.\n",
    "Remember that the student model can be thought of as a simplified (or compressed) version of the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac8d877-c8ee-43ea-afc0-ddc89a922d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Create the teacher\n",
    "teacher = create_big_model()\n",
    "\n",
    "# Plot architecture\n",
    "keras.utils.plot_model(teacher, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f20e302-1fc6-4098-862a-8a15d2dd9dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Create the student\n",
    "student = create_small_model()\n",
    "\n",
    "# Plot architecture\n",
    "keras.utils.plot_model(student, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80012ba-bd58-4e16-8ca6-0ad28eef2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model has: 9568898 trainable params.\n",
      "\n",
      "Student model has: 789442 trainable params.\n",
      "\n",
      "Teacher model is roughly 12 times bigger than the student model.\n"
     ]
    }
   ],
   "source": [
    "# Calculates number of trainable params for a given model\n",
    "def num_trainable_params(model):\n",
    "  return np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
    "\n",
    "\n",
    "student_params = num_trainable_params(student)\n",
    "teacher_params = num_trainable_params(teacher)\n",
    "\n",
    "print(f\"Teacher model has: {teacher_params} trainable params.\\n\")\n",
    "print(f\"Student model has: {student_params} trainable params.\\n\")\n",
    "print(f\"Teacher model is roughly {teacher_params//student_params} times bigger than the student model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dfa9a-024a-4718-b745-801d85186e2b",
   "metadata": {},
   "source": [
    "Train the teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0040de-9c40-49d7-be04-a7e6beb10c1d",
   "metadata": {},
   "source": [
    "In knowledge distillation it is assumed that the teacher has already been trained so the natural first step is to train the teacher. You will do so for a total of 8 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40440606-bcbb-40a1-8f6e-876ae9808186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "582/582 [==============================] - 324s 554ms/step - loss: 0.6770 - sparse_categorical_accuracy: 0.5736 - val_loss: 0.6211 - val_sparse_categorical_accuracy: 0.6870\n",
      "Epoch 2/8\n",
      "582/582 [==============================] - 326s 560ms/step - loss: 0.5704 - sparse_categorical_accuracy: 0.7006 - val_loss: 0.5344 - val_sparse_categorical_accuracy: 0.7446\n",
      "Epoch 3/8\n",
      "582/582 [==============================] - 317s 544ms/step - loss: 0.5015 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.5848 - val_sparse_categorical_accuracy: 0.7034\n",
      "Epoch 4/8\n",
      "582/582 [==============================] - 37441s 64s/step - loss: 0.4347 - sparse_categorical_accuracy: 0.8008 - val_loss: 0.4027 - val_sparse_categorical_accuracy: 0.8160\n",
      "Epoch 5/8\n",
      "582/582 [==============================] - 335s 575ms/step - loss: 0.3896 - sparse_categorical_accuracy: 0.8259 - val_loss: 0.4005 - val_sparse_categorical_accuracy: 0.8130\n",
      "Epoch 6/8\n",
      "582/582 [==============================] - 312s 535ms/step - loss: 0.3478 - sparse_categorical_accuracy: 0.8455 - val_loss: 0.3734 - val_sparse_categorical_accuracy: 0.8276\n",
      "Epoch 7/8\n",
      "582/582 [==============================] - 318s 546ms/step - loss: 0.3144 - sparse_categorical_accuracy: 0.8637 - val_loss: 0.3741 - val_sparse_categorical_accuracy: 0.8375\n",
      "Epoch 8/8\n",
      "582/582 [==============================] - 296s 508ms/step - loss: 0.2734 - sparse_categorical_accuracy: 0.8838 - val_loss: 0.4306 - val_sparse_categorical_accuracy: 0.8095\n"
     ]
    }
   ],
   "source": [
    "# Compile the teacher model\n",
    "teacher.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Notice from_logits param is set to True\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Fit the model and save the training history (will take from 5 to 10 minutes depending on the GPU you were assigned to)\n",
    "teacher_history = teacher.fit(train_batches, epochs=8, validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b0179-8f71-4ca7-a4d1-dee550bc7832",
   "metadata": {},
   "source": [
    "TRAIN A STUDENT FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762d959-45e6-4cdb-abd3-2ad596789fc8",
   "metadata": {},
   "source": [
    "In order to assess the effectiveness of the distillation process, train a model that is equivalent to the student but without doing knowledge distillation. Notice that the training is done for only 5 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f1d6f-76a7-4edc-8c9b-1cb584712654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student_scratch model with the same characteristics as the original student\n",
    "student_scratch = create_small_model()\n",
    "\n",
    "# Compile it || Overwriting the keras version\n",
    "student_scratch.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Train and evaluate student trained from scratch (will take around 3 mins with GPU enabled)\n",
    "student_scratch_history = student_scratch.fit(train_batches, epochs=5, validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f6cf2-8b1b-4f19-93f3-c8f6cfceeaf5",
   "metadata": {},
   "source": [
    "Knowledge Distillation\n",
    "To perform the knowledge distillation process you will use the custom model you previously coded. To do so, begin by creating an instance of the Distiller class and passing in the student and teacher models. Then compile it with the appropiate parameters and train it!\n",
    "\n",
    "The two student models are trained for only 5 epochs unlike the teacher that was trained for 8. This is done to showcase that the knowledge distillation allows for quicker training times as the student learns from an already trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2c487-def9-4f8e-aba9-8c4df7665f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Distiller instance\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "\n",
    "# Compile Distiller model\n",
    "distiller.compile(\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.05,\n",
    "    temperature=5,\n",
    ")\n",
    "\n",
    "# Distill knowledge from teacher to student (will take around 3 mins with GPU enabled)\n",
    "distiller_history = distiller.fit(train_batches, epochs=5, validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b7492-a9dd-4eb6-a3ae-70ffab9d83f4",
   "metadata": {},
   "source": [
    "Comparing the models\n",
    "To compare the models you can check the sparse_categorical_accuracy of each one on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba590f8-91fd-4fc7-9f0f-dbf0c4e7a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracies\n",
    "student_scratch_acc = student_scratch.evaluate(test_batches, return_dict=True).get(\"sparse_categorical_accuracy\")\n",
    "distiller_acc = distiller.evaluate(test_batches, return_dict=True).get(\"sparse_categorical_accuracy\")\n",
    "teacher_acc = teacher.evaluate(test_batches, return_dict=True).get(\"sparse_categorical_accuracy\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n\\nTeacher achieved a sparse_categorical_accuracy of {teacher_acc*100:.2f}%.\\n\")\n",
    "print(f\"Student with knowledge distillation achieved a sparse_categorical_accuracy of {distiller_acc*100:.2f}%.\\n\")\n",
    "print(f\"Student without knowledge distillation achieved a sparse_categorical_accuracy of {student_scratch_acc*100:.2f}%.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1640a97-47a3-4096-a298-36f5ab2b0f42",
   "metadata": {},
   "source": [
    "The teacher model yields a higger accuracy than the two student models. This is expected since it was trained for more epochs while using a bigger architecture.\n",
    "\n",
    "Notice that the student without distillation was outperfomed by the student with knowledge distillation.\n",
    "\n",
    "Since you saved the training history of each model you can create a plot for a better comparison of the two student models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67689d-ee3b-4150-94d2-601a27411e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant metrics from a history\n",
    "def get_metrics(history):\n",
    "  history = history.history\n",
    "  acc = history['sparse_categorical_accuracy']\n",
    "  val_acc = history['val_sparse_categorical_accuracy']\n",
    "  return acc, val_acc\n",
    "\n",
    "\n",
    "# Plot training and evaluation metrics given a dict of histories\n",
    "def plot_train_eval(history_dict):\n",
    "  \n",
    "  metric_dict = {}\n",
    "\n",
    "  for k, v in history_dict.items():\n",
    "    acc, val_acc= get_metrics(v)\n",
    "    metric_dict[f'{k} training acc'] = acc\n",
    "    metric_dict[f'{k} eval acc'] = val_acc\n",
    "\n",
    "  acc_plot = pd.DataFrame(metric_dict)\n",
    "  \n",
    "  acc_plot = sns.lineplot(data=acc_plot, markers=True)\n",
    "  acc_plot.set_title('training vs evaluation accuracy')\n",
    "  acc_plot.set_xlabel('epoch')\n",
    "  acc_plot.set_ylabel('sparse_categorical_accuracy')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Plot for comparing the two student models\n",
    "plot_train_eval({\n",
    "    \"distilled\": distiller_history,\n",
    "    \"student_scratch\": student_scratch_history,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c36e4-c398-4d0c-b5c2-d031f245f006",
   "metadata": {},
   "source": [
    "Result:\n",
    " the distilled version outperformed the unmodified one in almost all of the epochs when using the evaluation set. Alongside this, the student without distillation yields a bigger training accuracy, which is a sign that it is overfitting more than the distilled model. This hints that the distilled model was able to learn from the regularization that the teacher implemented!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71feeb-3740-45a5-b801-e8610ffe4d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81e6aa-dddf-4b1e-b16b-eb07018b9688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddb977-9697-4ffa-8802-b2681fbe6053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d4632-06bc-42b0-8070-b0c8dba8dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738f80d-f89b-4e7c-b83a-dd02554b9b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f52124-6bf4-4a02-8f55-84a223071d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
